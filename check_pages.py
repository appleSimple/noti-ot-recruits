import json
import os
import re
import time
from dataclasses import dataclass
from typing import Dict, List, Set
from urllib.parse import urljoin

import requests
from bs4 import BeautifulSoup
from requests.adapters import HTTPAdapter
from urllib3.util.retry import Retry

CONFIG_FILE = "targets.json"
STATE_FILE = "state.json"

BOT_TOKEN = os.environ.get("BOT_TOKEN", "").strip()
CHAT_ID = os.environ.get("CHAT_ID", "").strip()

HEADERS = {
    "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
    "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8",
    "Accept-Language": "ko-KR,ko;q=0.9,en-US;q=0.8,en;q=0.7",
    "Accept-Encoding": "gzip, deflate, br",
    "Connection": "keep-alive",
    "Upgrade-Insecure-Requests": "1",
}

def get_session():
    """ì¬ì‹œë„ ë¡œì§ì´ í¬í•¨ëœ requests ì„¸ì…˜ ìƒì„±"""
    session = requests.Session()
    
    # ì¬ì‹œë„ ì „ëµ: ì´ 5íšŒ, ì—°ê²° ì—ëŸ¬/ì½ê¸° íƒ€ì„ì•„ì›ƒ ì‹œ ì¬ì‹œë„
    retry_strategy = Retry(
        total=5,
        backoff_factor=3,  # 3ì´ˆ, 6ì´ˆ, 12ì´ˆ, 24ì´ˆ, 48ì´ˆ ê°„ê²©ìœ¼ë¡œ ì¬ì‹œë„
        status_forcelist=[429, 500, 502, 503, 504],
        allowed_methods=["GET", "POST"],
        raise_on_status=False  # ìƒíƒœ ì½”ë“œ ì—ëŸ¬ë¥¼ ë°”ë¡œ ë°œìƒì‹œí‚¤ì§€ ì•Šê³  ì¬ì‹œë„
    )
    
    adapter = HTTPAdapter(max_retries=retry_strategy)
    session.mount("http://", adapter)
    session.mount("https://", adapter)
    
    return session

@dataclass
class Item:
    item_id: str   # ëª©ë¡ ê¸€ë²ˆí˜¸(ìˆ«ì)
    title: str
    url: str       # ë”¥ë§í¬ê°€ ìˆìœ¼ë©´ ë”¥ë§í¬, ì—†ìœ¼ë©´ ëª©ë¡ URL

def load_config() -> Dict:
    with open(CONFIG_FILE, "r", encoding="utf-8") as f:
        return json.load(f)

def load_state() -> Dict[str, Set[str]]:
    if not os.path.exists(STATE_FILE):
        return {}
    try:
        with open(STATE_FILE, "r", encoding="utf-8") as f:
            raw = json.load(f)
        return {k: set(map(str, v)) for k, v in raw.items()}
    except Exception:
        return {}

def save_state(state: Dict[str, Set[str]]):
    compact = {k: list(sorted(v, reverse=True))[:3000] for k, v in state.items()}
    with open(STATE_FILE, "w", encoding="utf-8") as f:
        json.dump(compact, f, ensure_ascii=False, indent=2)

def telegram_send(text: str):
    if not BOT_TOKEN or not CHAT_ID:
        raise RuntimeError("BOT_TOKEN / CHAT_ID í™˜ê²½ë³€ìˆ˜ê°€ ë¹„ì–´ ìˆìŠµë‹ˆë‹¤. (GitHub Secrets í™•ì¸)")

    api = f"https://api.telegram.org/bot{BOT_TOKEN}/sendMessage"
    payload = {
        "chat_id": CHAT_ID,
        "text": text,
        "disable_web_page_preview": True,
    }
    r = requests.post(api, json=payload, timeout=20)
    r.raise_for_status()

def fetch_html(url: str) -> tuple[str, str]:
    """
    returns: (final_url, html_text)
    - ì¸ì½”ë”© ë³´ì • í¬í•¨
    - ì¬ì‹œë„ ë¡œì§ í¬í•¨
    """
    session = get_session()
    
    # ì‚¬ì´íŠ¸ë³„ íŠ¹ë³„ ì²˜ë¦¬
    headers = HEADERS.copy()
    timeout = (15, 45)  # (ì—°ê²°, ì½ê¸°)
    
    # 403 ì°¨ë‹¨ ìš°íšŒ ì‹œë„: Referer ì¶”ê°€
    if "jwf.or.kr" in url:
        headers["Referer"] = "http://www.jwf.or.kr/"
        # ë” ì¼ë°˜ì ì¸ User-Agent ì‚¬ìš©
        headers["User-Agent"] = "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36"
    
    # ì—°ê²°ì´ ëŠë¦° ì‚¬ì´íŠ¸: íƒ€ì„ì•„ì›ƒ ì¦ê°€
    if "hs4u.or.kr" in url or "hscity.go.kr" in url:
        timeout = (30, 60)
    
    r = session.get(url, headers=headers, timeout=timeout, allow_redirects=True)
    r.raise_for_status()

    # ì¸ì½”ë”© ë³´ì • (íŠ¹íˆ EUC-KR/CP949 ì‚¬ì´íŠ¸)
    if not r.encoding or (r.encoding.lower() in ["iso-8859-1", "latin-1"]):
        r.encoding = r.apparent_encoding or r.encoding

    return r.url, r.text

def parse_nid_or_kr(soup: BeautifulSoup, base_url: str, latest_n: int, debug: bool = False) -> List[Item]:
    """ì¹˜ë§¤ì•ˆì‹¬ì„¼í„°: recruit_view.aspx?no=XXX í˜•ì‹"""
    items_by_id: Dict[str, Item] = {}
    
    if debug:
        print(f"  [DEBUG] ì¹˜ë§¤ì•ˆì‹¬ì„¼í„° íŒŒì„œ ì‹¤í–‰")
    
    # recruit_view.aspx?no= ë§í¬ ì°¾ê¸°
    for a in soup.find_all("a", href=True):
        href = a.get("href", "")
        if "recruit_view.aspx" in href and "no=" in href:
            # no= íŒŒë¼ë¯¸í„° ì¶”ì¶œ
            no_match = re.search(r'[?&]no=(\d+)', href)
            if not no_match:
                continue
            
            item_id = no_match.group(1)
            title = a.get_text(strip=True)
            
            # [ì±„ìš©ì¤‘] ê°™ì€ íƒœê·¸ ì œê±°
            title = re.sub(r'\[ì±„ìš©ì¤‘\]|\[ì±„ìš©ì¢…ë£Œ\]', '', title).strip()
            
            if not title:
                continue
            
            full_url = urljoin(base_url, href)
            items_by_id[item_id] = Item(item_id=item_id, title=title, url=full_url)
    
    if debug:
        print(f"  [DEBUG] ì¹˜ë§¤ì•ˆì‹¬ì„¼í„°: {len(items_by_id)}ê°œ í•­ëª© ë°œê²¬")
    
    items = sorted(items_by_id.values(), key=lambda it: int(it.item_id), reverse=True)
    return items[:latest_n]

def parse_health_suwon(soup: BeautifulSoup, base_url: str, latest_n: int, debug: bool = False) -> List[Item]:
    """ìˆ˜ì›ì‹œë³´ê±´ì†Œ: URLì˜ no= íŒŒë¼ë¯¸í„° ì¶”ì¶œ"""
    items_by_id: Dict[str, Item] = {}
    
    if debug:
        print(f"  [DEBUG] ìˆ˜ì›ì‹œë³´ê±´ì†Œ íŒŒì„œ ì‹¤í–‰")
    
    for a in soup.find_all("a", href=True):
        href = a.get("href", "")
        if "board_view.asp" in href and "no=" in href:
            # no= íŒŒë¼ë¯¸í„° ì¶”ì¶œ
            no_match = re.search(r'[?&]no=(\d+)', href)
            if not no_match:
                continue
            
            item_id = no_match.group(1)
            title = a.get_text(strip=True)
            
            if not title:
                continue
            
            full_url = urljoin(base_url, href)
            items_by_id[item_id] = Item(item_id=item_id, title=title, url=full_url)
    
    if debug:
        print(f"  [DEBUG] ìˆ˜ì›ì‹œë³´ê±´ì†Œ: {len(items_by_id)}ê°œ í•­ëª© ë°œê²¬")
    
    items = sorted(items_by_id.values(), key=lambda it: int(it.item_id), reverse=True)
    return items[:latest_n]

def parse_hs4u(soup: BeautifulSoup, base_url: str, latest_n: int, debug: bool = False) -> List[Item]:
    """í™”ì„±ì‹œì¥ì• ì•„ë™ì¬í™œì„¼í„°: seq= íŒŒë¼ë¯¸í„° ì¶”ì¶œ"""
    items_by_id: Dict[str, Item] = {}
    
    if debug:
        print(f"  [DEBUG] í™”ì„±ì‹œì¥ì• ì•„ë™ì¬í™œì„¼í„° íŒŒì„œ ì‹¤í–‰")
    
    for a in soup.find_all("a", href=True):
        href = a.get("href", "")
        if "subAct=view" in href and "seq=" in href:
            # seq= íŒŒë¼ë¯¸í„° ì¶”ì¶œ
            seq_match = re.search(r'[?&]seq=(\d+)', href)
            if not seq_match:
                continue
            
            item_id = seq_match.group(1)
            title = a.get_text(strip=True)
            
            # ì•„ì´ì½˜ í…ìŠ¤íŠ¸ ì œê±°
            title = title.replace('[ìƒˆê¸€]', '').replace('[ì´ë¯¸ì§€]', '').replace('[ë‹¤ìš´ë¡œë“œ]', '').strip()
            
            if not title:
                continue
            
            full_url = urljoin(base_url, href)
            items_by_id[item_id] = Item(item_id=item_id, title=title, url=full_url)
    
    if debug:
        print(f"  [DEBUG] í™”ì„±ì‹œì¥ì• ì•„ë™ì¬í™œì„¼í„°: {len(items_by_id)}ê°œ í•­ëª© ë°œê²¬")
    
    items = sorted(items_by_id.values(), key=lambda it: int(it.item_id), reverse=True)
    return items[:latest_n]

def parse_html_list_number_id(target_url: str, latest_n: int, debug: bool = False) -> List[Item]:
    """
    ëª©ë¡ì—ì„œ ê¸€ë²ˆí˜¸(ìˆ«ì)ë¥¼ item_idë¡œ ì‚¬ìš©.
    ì „í˜•ì ì¸ í…Œì´ë¸” ëª©ë¡:
      <tr>
        <td>376</td>
        <td><a ...>ì œëª©</a></td>
        ...
      </tr>

    URL:
    - a[href]ê°€ ì‹¤ë§í¬ë©´ urljoiní•´ì„œ ì‚¬ìš©
    - hrefê°€ #/javascriptë©´ onclickì—ì„œ '...' í˜•íƒœ URLì´ ìˆìœ¼ë©´ ì¶”ì¶œ
    - ê·¸ë§ˆì €ë„ ì—†ìœ¼ë©´ target_url(ëª©ë¡) ì‚¬ìš©
    """
    final_url, html = fetch_html(target_url)
    soup = BeautifulSoup(html, "lxml")

    items_by_id: Dict[str, Item] = {}
    
    # ì‚¬ì´íŠ¸ë³„ íŠ¹ë³„ íŒŒì„œ
    if "nid.or.kr" in target_url:
        # ì¹˜ë§¤ì•ˆì‹¬ì„¼í„°: recruit_view.aspx?no=XXX í˜•ì‹
        return parse_nid_or_kr(soup, final_url, latest_n, debug)
    elif "health.suwon.go.kr" in target_url:
        # ìˆ˜ì›ì‹œë³´ê±´ì†Œ: URLì—ì„œ no= íŒŒë¼ë¯¸í„° ì¶”ì¶œ
        return parse_health_suwon(soup, final_url, latest_n, debug)
    elif "hs4u.or.kr" in target_url:
        # í™”ì„±ì‹œì¥ì• ì•„ë™ì¬í™œì„¼í„°: seq= íŒŒë¼ë¯¸í„° ì¶”ì¶œ
        return parse_hs4u(soup, final_url, latest_n, debug)

    # ë””ë²„ê·¸ ëª¨ë“œ: HTML êµ¬ì¡° ì¶œë ¥
    if debug:
        print(f"  [DEBUG] HTML ê¸¸ì´: {len(html)}")
        trs = soup.find_all("tr")
        print(f"  [DEBUG] ì´ tr ê°œìˆ˜: {len(trs)}")
        for i, tr in enumerate(trs[:10]):  # ì²˜ìŒ 10ê°œë§Œ
            tds = tr.find_all("td")
            if tds:
                td_texts = [td.get_text(strip=True)[:50] for td in tds[:5]]
                print(f"  [DEBUG] tr[{i}] - td ê°œìˆ˜: {len(tds)}, ë‚´ìš©: {td_texts}")

    # 1) ê°€ì¥ ì•ˆì •ì ì¸ íŒ¨í„´: trì˜ ì²« tdê°€ ìˆ«ì
    for tr in soup.find_all("tr"):
        tds = tr.find_all("td")
        if not tds:
            continue

        no = tds[0].get_text(strip=True)
        if not no.isdigit():
            continue

        a = tr.find("a")
        if not a:
            continue

        title = a.get_text(strip=True)
        if not title:
            continue

        href = (a.get("href") or "").strip()
        onclick = (a.get("onclick") or "").strip()

        full_url = target_url  # fallbackì€ ëª©ë¡
        if href and href not in ["#", "javascript:void(0);", "javascript:void(0)"] and not href.lower().startswith("javascript:"):
            full_url = urljoin(final_url, href)
        else:
            # onclickì— URL ë¬¸ìì—´ì´ ë“¤ì–´ìˆëŠ” ê²½ìš°: 'view.php?...' ë˜ëŠ” "/path/..." ë“±
            url_m = re.search(r"""['"]([^'"]+)['"]""", onclick)
            if url_m:
                full_url = urljoin(final_url, url_m.group(1))

        items_by_id[no] = Item(item_id=no, title=title, url=full_url)

    # 2) í˜¹ì‹œ í…Œì´ë¸” êµ¬ì¡°ê°€ ë‹¬ë¼ì„œ 1)ì´ ë¹„ë©´: aì˜ ë¶€ëª¨ trì—ì„œ ì²« td ìˆ«ì ì°¾ê¸°
    if not items_by_id:
        for a in soup.find_all("a"):
            title = a.get_text(strip=True)
            if not title:
                continue

            tr = a.find_parent("tr")
            if not tr:
                continue
            tds = tr.find_all("td")
            if not tds:
                continue

            no = tds[0].get_text(strip=True)
            if not no.isdigit():
                continue

            href = (a.get("href") or "").strip()
            full_url = urljoin(final_url, href) if href and not href.lower().startswith("javascript:") else target_url
            items_by_id[no] = Item(item_id=no, title=title, url=full_url)

    # 3) ì—¬ì „íˆ ë¹„ì–´ìˆìœ¼ë©´ ë‹¤ë¥¸ íŒ¨í„´ ì‹œë„: tdì˜ ìˆœì„œê°€ ë‹¤ë¥¼ ìˆ˜ ìˆìŒ
    if not items_by_id and debug:
        print(f"  [DEBUG] íŒ¨í„´ 1, 2 ì‹¤íŒ¨. ë‹¤ë¥¸ íŒ¨í„´ íƒìƒ‰ ì¤‘...")

    items = sorted(items_by_id.values(), key=lambda it: int(it.item_id), reverse=True)
    return items[:latest_n]

def run_target(target: Dict, state: Dict[str, Set[str]]):
    name = str(target.get("name", "unknown"))
    url = target["url"]
    ttype = target.get("type", "html_list_number_id")
    latest_n = int(target.get("latest_n", 30))

    if ttype != "html_list_number_id":
        raise ValueError(f"Unsupported target type (only html_list_number_id): {ttype}")

    seen = state.get(name, set())

    items = parse_html_list_number_id(url, latest_n, debug=False)

    print(f"[{name}] fetched={len(items)} first5={[ (it.item_id, it.title) for it in items[:5] ]}")

    # íŒŒì‹± ì‹¤íŒ¨ ê°ì§€ - ë””ë²„ê·¸ ëª¨ë“œë¡œ ì¬ì‹œë„
    if not items:
        print(f"âš ï¸ [{name}] íŒŒì‹± ì‹¤íŒ¨: ê¸€ ëª©ë¡ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ë””ë²„ê·¸ ëª¨ë“œë¡œ ì¬ì‹œë„...")
        items = parse_html_list_number_id(url, latest_n, debug=True)
        
        if not items:
            print(f"âš ï¸ [{name}] ë””ë²„ê·¸ ëª¨ë“œì—ì„œë„ íŒŒì‹± ì‹¤íŒ¨.")
            # íŒŒì‹± ì‹¤íŒ¨ ì‹œì—ë„ í…”ë ˆê·¸ë¨ìœ¼ë¡œ ì•Œë¦¼
            if BOT_TOKEN and CHAT_ID:
                try:
                    telegram_send(f"âš ï¸ íŒŒì‹± ì‹¤íŒ¨ ({name})\n- URL: {url}\n- ê¸€ ëª©ë¡ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
                except:
                    pass
            return

    new_items = [it for it in items if it.item_id not in seen]
    if not new_items:
        print(f"[{name}] No new items.")
        return

    # ì˜¤ë˜ëœ ê²ƒë¶€í„° ì•Œë¦¼ ë³´ë‚´ê¸°
    new_items.sort(key=lambda it: int(it.item_id))

    for it in new_items:
        msg = f"ğŸ†• ìƒˆ ê¸€ ({name})\n- {it.title}\n- {it.url}"
        telegram_send(msg)
        print(f"[{name}] Sent: {it.item_id} {it.title}")
        seen.add(it.item_id)
        time.sleep(0.7)

    state[name] = seen

def main():
    config = load_config()
    targets = config.get("targets", [])
    if not targets:
        raise RuntimeError("targets.jsonì— targetsê°€ ë¹„ì–´ ìˆìŠµë‹ˆë‹¤.")

    state = load_state()
    errors = []

    for target in targets:
        try:
            run_target(target, state)
        except Exception as e:
            err_msg = f"âš ï¸ í¬ë¡¤ëŸ¬ ì˜¤ë¥˜ ({target.get('name','unknown')})\n- {type(e).__name__}: {e}"
            print(err_msg)
            errors.append(err_msg)

    save_state(state)
    
    # ì—ëŸ¬ê°€ ìˆìœ¼ë©´ í…”ë ˆê·¸ë¨ìœ¼ë¡œ ì•Œë¦¼ (ì„ íƒì )
    if errors and BOT_TOKEN and CHAT_ID:
        try:
            summary = "\n\n".join(errors)
            telegram_send(f"ğŸ“‹ í¬ë¡¤ëŸ¬ ì‹¤í–‰ ì™„ë£Œ ({len(errors)}ê°œ ì—ëŸ¬ ë°œìƒ)\n\n{summary}")
        except Exception as e:
            print(f"í…”ë ˆê·¸ë¨ ì—ëŸ¬ ì•Œë¦¼ ì „ì†¡ ì‹¤íŒ¨: {e}")

if __name__ == "__main__":
    main()