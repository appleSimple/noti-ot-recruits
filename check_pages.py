import json
import os
import re
import time
from dataclasses import dataclass
from typing import Dict, List, Set
from urllib.parse import urljoin

import requests
from bs4 import BeautifulSoup
from requests.adapters import HTTPAdapter
from urllib3.util.retry import Retry

CONFIG_FILE = "targets.json"
STATE_FILE = "state.json"

BOT_TOKEN = os.environ.get("BOT_TOKEN", "").strip()
CHAT_ID = os.environ.get("CHAT_ID", "").strip()

HEADERS = {
    "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/131.0.0.0 Safari/537.36",
    "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.7",
    "Accept-Language": "ko-KR,ko;q=0.9,en-US;q=0.8,en;q=0.7",
    "Accept-Encoding": "gzip, deflate, br",
    "Connection": "keep-alive",
    "Upgrade-Insecure-Requests": "1",
    "Cache-Control": "max-age=0",
    "sec-ch-ua": '"Google Chrome";v="131", "Chromium";v="131", "Not_A Brand";v="24"',
    "sec-ch-ua-mobile": "?0",
    "sec-ch-ua-platform": '"Windows"',
    "Sec-Fetch-Dest": "document",
    "Sec-Fetch-Mode": "navigate",
    "Sec-Fetch-Site": "none",
    "Sec-Fetch-User": "?1",
}

def get_session():
    """ì¬ì‹œë„ ë¡œì§ì´ í¬í•¨ëœ requests ì„¸ì…˜ ìƒì„±"""
    session = requests.Session()
    
    # ì¬ì‹œë„ ì „ëµ: ì´ 8íšŒë¡œ ì¦ê°€, ë” ë§ì€ ìƒíƒœ ì½”ë“œ ì²˜ë¦¬
    retry_strategy = Retry(
        total=8,
        backoff_factor=2,  # 2ì´ˆ, 4ì´ˆ, 8ì´ˆ, 16ì´ˆ... ê°„ê²©ìœ¼ë¡œ ì¬ì‹œë„
        status_forcelist=[403, 408, 429, 500, 502, 503, 504],  # 403ë„ ì¬ì‹œë„
        allowed_methods=["GET", "POST"],
        raise_on_status=False
    )
    
    adapter = HTTPAdapter(max_retries=retry_strategy, pool_connections=10, pool_maxsize=20)
    session.mount("http://", adapter)
    session.mount("https://", adapter)
    
    return session

@dataclass
class Item:
    item_id: str   # ëª©ë¡ ê¸€ë²ˆí˜¸(ìˆ«ì)
    title: str
    url: str       # ë”¥ë§í¬ê°€ ìˆìœ¼ë©´ ë”¥ë§í¬, ì—†ìœ¼ë©´ ëª©ë¡ URL

def load_config() -> Dict:
    with open(CONFIG_FILE, "r", encoding="utf-8") as f:
        return json.load(f)

def load_state() -> Dict[str, Set[str]]:
    if not os.path.exists(STATE_FILE):
        return {}
    try:
        with open(STATE_FILE, "r", encoding="utf-8") as f:
            raw = json.load(f)
        return {k: set(map(str, v)) for k, v in raw.items()}
    except Exception:
        return {}

def save_state(state: Dict[str, Set[str]]):
    compact = {k: list(sorted(v, reverse=True))[:3000] for k, v in state.items()}
    with open(STATE_FILE, "w", encoding="utf-8") as f:
        json.dump(compact, f, ensure_ascii=False, indent=2)

def telegram_send(text: str):
    if not BOT_TOKEN or not CHAT_ID:
        raise RuntimeError("BOT_TOKEN / CHAT_ID í™˜ê²½ë³€ìˆ˜ê°€ ë¹„ì–´ ìˆìŠµë‹ˆë‹¤. (GitHub Secrets í™•ì¸)")

    api = f"https://api.telegram.org/bot{BOT_TOKEN}/sendMessage"
    payload = {
        "chat_id": CHAT_ID,
        "text": text,
        "disable_web_page_preview": True,
    }
    r = requests.post(api, json=payload, timeout=20)
    r.raise_for_status()

def fetch_html(url: str, retry_count: int = 0) -> tuple[str, str]:
    """
    returns: (final_url, html_text)
    - ì¸ì½”ë”© ë³´ì • í¬í•¨
    - ì¬ì‹œë„ ë¡œì§ í¬í•¨
    - retry_count: ìˆ˜ë™ ì¬ì‹œë„ íšŸìˆ˜ (ë‚´ë¶€ìš©)
    """
    session = get_session()
    
    # ì‚¬ì´íŠ¸ë³„ íŠ¹ë³„ ì²˜ë¦¬
    headers = HEADERS.copy()
    timeout = (20, 60)  # ê¸°ë³¸ íƒ€ì„ì•„ì›ƒ ì¦ê°€: (ì—°ê²° 20ì´ˆ, ì½ê¸° 60ì´ˆ)
    
    # ì§€êµ¬ì´Œì‚¬íšŒë³µì§€ì¬ë‹¨: 403 ì°¨ë‹¨ ìš°íšŒ
    if "jwf.or.kr" in url:
        headers["Referer"] = "http://www.jwf.or.kr/"
        headers["Origin"] = "http://www.jwf.or.kr"
        # ë‹¤ì–‘í•œ User-Agent ì‹œë„
        user_agents = [
            "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/131.0.0.0 Safari/537.36",
            "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/131.0.0.0 Safari/537.36",
            "Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/131.0.0.0 Safari/537.36",
        ]
        headers["User-Agent"] = user_agents[retry_count % len(user_agents)]
        timeout = (30, 90)
    
    # í™”ì„±ì‹œì¥ì• ì•„ë™ì¬í™œì„¼í„°: ì—°ê²° íƒ€ì„ì•„ì›ƒ ëŒ€ë¹„
    if "hs4u.or.kr" in url:
        timeout = (45, 90)  # íƒ€ì„ì•„ì›ƒ ëŒ€í­ ì¦ê°€
        headers["Referer"] = "https://www.hs4u.or.kr/"
    
    # ì¹˜ë§¤ì•ˆì‹¬ì„¼í„°: ASP.NET í˜ì´ì§€
    if "nid.or.kr" in url:
        timeout = (30, 90)
        headers["Referer"] = "https://www.nid.or.kr/"
    
    # ìˆ˜ì›ì‹œë³´ê±´ì†Œ: ASP í˜ì´ì§€
    if "health.suwon.go.kr" in url:
        timeout = (30, 90)
        headers["Referer"] = "https://health.suwon.go.kr/"
    
    try:
        r = session.get(url, headers=headers, timeout=timeout, allow_redirects=True)
        r.raise_for_status()
        
        # ì¸ì½”ë”© ë³´ì • (íŠ¹íˆ EUC-KR/CP949 ì‚¬ì´íŠ¸)
        if not r.encoding or (r.encoding.lower() in ["iso-8859-1", "latin-1"]):
            r.encoding = r.apparent_encoding or r.encoding
        
        return r.url, r.text
        
    except requests.exceptions.Timeout as e:
        # íƒ€ì„ì•„ì›ƒ ë°œìƒ ì‹œ í•œ ë²ˆ ë” ì¬ì‹œë„ (ìµœëŒ€ 2íšŒ)
        if retry_count < 2:
            print(f"  [ì¬ì‹œë„ {retry_count + 1}/2] íƒ€ì„ì•„ì›ƒ ë°œìƒ, ì¬ì‹œë„ ì¤‘...")
            time.sleep(5)  # 5ì´ˆ ëŒ€ê¸° í›„ ì¬ì‹œë„
            return fetch_html(url, retry_count + 1)
        else:
            raise
    
    except requests.exceptions.HTTPError as e:
        # 403 ë“± HTTP ì—ëŸ¬ ì‹œ í•œ ë²ˆ ë” ì¬ì‹œë„
        if e.response.status_code == 403 and retry_count < 2:
            print(f"  [ì¬ì‹œë„ {retry_count + 1}/2] 403 ì˜¤ë¥˜, User-Agent ë³€ê²½ í›„ ì¬ì‹œë„...")
            time.sleep(3)
            return fetch_html(url, retry_count + 1)
        else:
            raise

def parse_nid_or_kr(soup: BeautifulSoup, base_url: str, latest_n: int, debug: bool = False) -> List[Item]:
    """ì¹˜ë§¤ì•ˆì‹¬ì„¼í„°: recruit_view.aspx?no=XXX í˜•ì‹"""
    items_by_id: Dict[str, Item] = {}
    
    if debug:
        print(f"  [DEBUG] ì¹˜ë§¤ì•ˆì‹¬ì„¼í„° íŒŒì„œ ì‹¤í–‰")
        all_links = soup.find_all("a", href=True)
        print(f"  [DEBUG] ì „ì²´ ë§í¬ ê°œìˆ˜: {len(all_links)}")
        recruit_links = [a for a in all_links if "recruit" in a.get("href", "").lower()]
        print(f"  [DEBUG] recruit ê´€ë ¨ ë§í¬: {len(recruit_links)}")
        if recruit_links:
            for i, a in enumerate(recruit_links[:3]):
                print(f"  [DEBUG]   ë§í¬ {i+1}: {a.get('href', '')[:100]}")
    
    # recruit_view.aspx?no= ë§í¬ ì°¾ê¸°
    for a in soup.find_all("a", href=True):
        href = a.get("href", "")
        if "recruit_view.aspx" in href and "no=" in href:
            # no= íŒŒë¼ë¯¸í„° ì¶”ì¶œ
            no_match = re.search(r'[?&]no=(\d+)', href)
            if not no_match:
                continue
            
            item_id = no_match.group(1)
            title = a.get_text(strip=True)
            
            # [ì±„ìš©ì¤‘] ê°™ì€ íƒœê·¸ ì œê±°
            title = re.sub(r'\[ì±„ìš©ì¤‘\]|\[ì±„ìš©ì¢…ë£Œ\]', '', title).strip()
            
            if not title:
                continue
            
            full_url = urljoin(base_url, href)
            items_by_id[item_id] = Item(item_id=item_id, title=title, url=full_url)
    
    if debug:
        print(f"  [DEBUG] ì¹˜ë§¤ì•ˆì‹¬ì„¼í„°: {len(items_by_id)}ê°œ í•­ëª© ë°œê²¬")
        if items_by_id:
            print(f"  [DEBUG] ì²« ë²ˆì§¸ í•­ëª©: {list(items_by_id.values())[0]}")
    
    items = sorted(items_by_id.values(), key=lambda it: int(it.item_id), reverse=True)
    return items[:latest_n]

def parse_health_suwon(soup: BeautifulSoup, base_url: str, latest_n: int, debug: bool = False) -> List[Item]:
    """ìˆ˜ì›ì‹œë³´ê±´ì†Œ: URLì˜ no= íŒŒë¼ë¯¸í„° ì¶”ì¶œ"""
    items_by_id: Dict[str, Item] = {}
    
    if debug:
        print(f"  [DEBUG] ìˆ˜ì›ì‹œë³´ê±´ì†Œ íŒŒì„œ ì‹¤í–‰")
    
    for a in soup.find_all("a", href=True):
        href = a.get("href", "")
        if "board_view.asp" in href and "no=" in href:
            # no= íŒŒë¼ë¯¸í„° ì¶”ì¶œ
            no_match = re.search(r'[?&]no=(\d+)', href)
            if not no_match:
                continue
            
            item_id = no_match.group(1)
            title = a.get_text(strip=True)
            
            if not title:
                continue
            
            full_url = urljoin(base_url, href)
            items_by_id[item_id] = Item(item_id=item_id, title=title, url=full_url)
    
    if debug:
        print(f"  [DEBUG] ìˆ˜ì›ì‹œë³´ê±´ì†Œ: {len(items_by_id)}ê°œ í•­ëª© ë°œê²¬")
    
    items = sorted(items_by_id.values(), key=lambda it: int(it.item_id), reverse=True)
    return items[:latest_n]

def parse_hs4u(soup: BeautifulSoup, base_url: str, latest_n: int, debug: bool = False) -> List[Item]:
    """í™”ì„±ì‹œì¥ì• ì•„ë™ì¬í™œì„¼í„°: seq= íŒŒë¼ë¯¸í„° ì¶”ì¶œ"""
    items_by_id: Dict[str, Item] = {}
    
    if debug:
        print(f"  [DEBUG] í™”ì„±ì‹œì¥ì• ì•„ë™ì¬í™œì„¼í„° íŒŒì„œ ì‹¤í–‰")
    
    for a in soup.find_all("a", href=True):
        href = a.get("href", "")
        if "subAct=view" in href and "seq=" in href:
            # seq= íŒŒë¼ë¯¸í„° ì¶”ì¶œ
            seq_match = re.search(r'[?&]seq=(\d+)', href)
            if not seq_match:
                continue
            
            item_id = seq_match.group(1)
            title = a.get_text(strip=True)
            
            # ì•„ì´ì½˜ í…ìŠ¤íŠ¸ ì œê±°
            title = title.replace('[ìƒˆê¸€]', '').replace('[ì´ë¯¸ì§€]', '').replace('[ë‹¤ìš´ë¡œë“œ]', '').strip()
            
            if not title:
                continue
            
            full_url = urljoin(base_url, href)
            items_by_id[item_id] = Item(item_id=item_id, title=title, url=full_url)
    
    if debug:
        print(f"  [DEBUG] í™”ì„±ì‹œì¥ì• ì•„ë™ì¬í™œì„¼í„°: {len(items_by_id)}ê°œ í•­ëª© ë°œê²¬")
    
    items = sorted(items_by_id.values(), key=lambda it: int(it.item_id), reverse=True)
    return items[:latest_n]

def parse_html_list_number_id(target_url: str, latest_n: int, debug: bool = False) -> List[Item]:
    """
    ëª©ë¡ì—ì„œ ê¸€ë²ˆí˜¸(ìˆ«ì)ë¥¼ item_idë¡œ ì‚¬ìš©.
    ì „í˜•ì ì¸ í…Œì´ë¸” ëª©ë¡:
      <tr>
        <td>376</td>
        <td><a ...>ì œëª©</a></td>
        ...
      </tr>

    URL:
    - a[href]ê°€ ì‹¤ë§í¬ë©´ urljoiní•´ì„œ ì‚¬ìš©
    - hrefê°€ #/javascriptë©´ onclickì—ì„œ '...' í˜•íƒœ URLì´ ìˆìœ¼ë©´ ì¶”ì¶œ
    - ê·¸ë§ˆì €ë„ ì—†ìœ¼ë©´ target_url(ëª©ë¡) ì‚¬ìš©
    """
    final_url, html = fetch_html(target_url)
    soup = BeautifulSoup(html, "lxml")

    items_by_id: Dict[str, Item] = {}
    
    # ì‚¬ì´íŠ¸ë³„ íŠ¹ë³„ íŒŒì„œ
    if "nid.or.kr" in target_url:
        # ì¹˜ë§¤ì•ˆì‹¬ì„¼í„°: recruit_view.aspx?no=XXX í˜•ì‹
        return parse_nid_or_kr(soup, final_url, latest_n, debug)
    elif "health.suwon.go.kr" in target_url:
        # ìˆ˜ì›ì‹œë³´ê±´ì†Œ: URLì—ì„œ no= íŒŒë¼ë¯¸í„° ì¶”ì¶œ
        return parse_health_suwon(soup, final_url, latest_n, debug)
    elif "hs4u.or.kr" in target_url:
        # í™”ì„±ì‹œì¥ì• ì•„ë™ì¬í™œì„¼í„°: seq= íŒŒë¼ë¯¸í„° ì¶”ì¶œ
        return parse_hs4u(soup, final_url, latest_n, debug)

    # ë””ë²„ê·¸ ëª¨ë“œ: HTML êµ¬ì¡° ì¶œë ¥
    if debug:
        print(f"  [DEBUG] HTML ê¸¸ì´: {len(html)}")
        trs = soup.find_all("tr")
        print(f"  [DEBUG] ì´ tr ê°œìˆ˜: {len(trs)}")
        for i, tr in enumerate(trs[:10]):  # ì²˜ìŒ 10ê°œë§Œ
            tds = tr.find_all("td")
            if tds:
                td_texts = [td.get_text(strip=True)[:50] for td in tds[:5]]
                print(f"  [DEBUG] tr[{i}] - td ê°œìˆ˜: {len(tds)}, ë‚´ìš©: {td_texts}")

    # 1) ê°€ì¥ ì•ˆì •ì ì¸ íŒ¨í„´: trì˜ ì²« tdê°€ ìˆ«ì
    for tr in soup.find_all("tr"):
        tds = tr.find_all("td")
        if not tds:
            continue

        no = tds[0].get_text(strip=True)
        if not no.isdigit():
            continue

        a = tr.find("a")
        if not a:
            continue

        title = a.get_text(strip=True)
        if not title:
            continue

        href = (a.get("href") or "").strip()
        onclick = (a.get("onclick") or "").strip()

        full_url = target_url  # fallbackì€ ëª©ë¡
        if href and href not in ["#", "javascript:void(0);", "javascript:void(0)"] and not href.lower().startswith("javascript:"):
            full_url = urljoin(final_url, href)
        else:
            # onclickì— URL ë¬¸ìì—´ì´ ë“¤ì–´ìˆëŠ” ê²½ìš°: 'view.php?...' ë˜ëŠ” "/path/..." ë“±
            url_m = re.search(r"""['"]([^'"]+)['"]""", onclick)
            if url_m:
                full_url = urljoin(final_url, url_m.group(1))

        items_by_id[no] = Item(item_id=no, title=title, url=full_url)

    # 2) í˜¹ì‹œ í…Œì´ë¸” êµ¬ì¡°ê°€ ë‹¬ë¼ì„œ 1)ì´ ë¹„ë©´: aì˜ ë¶€ëª¨ trì—ì„œ ì²« td ìˆ«ì ì°¾ê¸°
    if not items_by_id:
        for a in soup.find_all("a"):
            title = a.get_text(strip=True)
            if not title:
                continue

            tr = a.find_parent("tr")
            if not tr:
                continue
            tds = tr.find_all("td")
            if not tds:
                continue

            no = tds[0].get_text(strip=True)
            if not no.isdigit():
                continue

            href = (a.get("href") or "").strip()
            full_url = urljoin(final_url, href) if href and not href.lower().startswith("javascript:") else target_url
            items_by_id[no] = Item(item_id=no, title=title, url=full_url)

    # 3) ì—¬ì „íˆ ë¹„ì–´ìˆìœ¼ë©´ ë‹¤ë¥¸ íŒ¨í„´ ì‹œë„: tdì˜ ìˆœì„œê°€ ë‹¤ë¥¼ ìˆ˜ ìˆìŒ
    if not items_by_id and debug:
        print(f"  [DEBUG] íŒ¨í„´ 1, 2 ì‹¤íŒ¨. ë‹¤ë¥¸ íŒ¨í„´ íƒìƒ‰ ì¤‘...")

    items = sorted(items_by_id.values(), key=lambda it: int(it.item_id), reverse=True)
    return items[:latest_n]

def run_target(target: Dict, state: Dict[str, Set[str]]):
    name = str(target.get("name", "unknown"))
    url = target["url"]
    ttype = target.get("type", "html_list_number_id")
    latest_n = int(target.get("latest_n", 30))

    if ttype != "html_list_number_id":
        raise ValueError(f"Unsupported target type (only html_list_number_id): {ttype}")

    seen = state.get(name, set())

    try:
        items = parse_html_list_number_id(url, latest_n, debug=False)
        
        print(f"[{name}] fetched={len(items)} first5={[ (it.item_id, it.title) for it in items[:5] ]}")

        # íŒŒì‹± ì‹¤íŒ¨ ê°ì§€ - ë””ë²„ê·¸ ëª¨ë“œë¡œ ì¬ì‹œë„
        if not items:
            print(f"âš ï¸ [{name}] íŒŒì‹± ì‹¤íŒ¨: ê¸€ ëª©ë¡ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ë””ë²„ê·¸ ëª¨ë“œë¡œ ì¬ì‹œë„...")
            items = parse_html_list_number_id(url, latest_n, debug=True)
            
            if not items:
                print(f"âš ï¸ [{name}] ë””ë²„ê·¸ ëª¨ë“œì—ì„œë„ íŒŒì‹± ì‹¤íŒ¨.")
                raise RuntimeError(f"íŒŒì‹± ì‹¤íŒ¨: ê¸€ ëª©ë¡ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤")

        new_items = [it for it in items if it.item_id not in seen]
        if not new_items:
            print(f"[{name}] No new items.")
            return

        # ì˜¤ë˜ëœ ê²ƒë¶€í„° ì•Œë¦¼ ë³´ë‚´ê¸°
        new_items.sort(key=lambda it: int(it.item_id))

        for it in new_items:
            msg = f"ğŸ†• ìƒˆ ê¸€ ({name})\n- {it.title}\n- {it.url}"
            telegram_send(msg)
            print(f"[{name}] Sent: {it.item_id} {it.title}")
            seen.add(it.item_id)
            time.sleep(0.7)

        state[name] = seen
        
    except requests.exceptions.Timeout as e:
        # íƒ€ì„ì•„ì›ƒ ì—ëŸ¬ë¥¼ ëª…í™•íˆ í‘œì‹œ
        raise requests.exceptions.Timeout(f"ì—°ê²° íƒ€ì„ì•„ì›ƒ: {url}") from e
    except requests.exceptions.HTTPError as e:
        # HTTP ì—ëŸ¬ë¥¼ ëª…í™•íˆ í‘œì‹œ
        status_code = e.response.status_code if hasattr(e, 'response') and e.response else 'unknown'
        raise requests.exceptions.HTTPError(f"HTTP {status_code} ì˜¤ë¥˜: {url}") from e

def main():
    config = load_config()
    targets = config.get("targets", [])
    if not targets:
        raise RuntimeError("targets.jsonì— targetsê°€ ë¹„ì–´ ìˆìŠµë‹ˆë‹¤.")

    state = load_state()
    errors = []

    for i, target in enumerate(targets):
        try:
            run_target(target, state)
            
            # ê° ì‚¬ì´íŠ¸ í¬ë¡¤ë§ ì‚¬ì´ì— ë”œë ˆì´ ì¶”ê°€ (ë§ˆì§€ë§‰ ì œì™¸)
            if i < len(targets) - 1:
                time.sleep(2)
                
        except Exception as e:
            err_msg = f"âš ï¸ í¬ë¡¤ëŸ¬ ì˜¤ë¥˜ ({target.get('name','unknown')})\n- {type(e).__name__}: {e}"
            print(err_msg)
            errors.append(err_msg)

    save_state(state)
    
    # ì—ëŸ¬ê°€ ìˆìœ¼ë©´ í…”ë ˆê·¸ë¨ìœ¼ë¡œ ì•Œë¦¼ (ì„ íƒì )
    if errors and BOT_TOKEN and CHAT_ID:
        try:
            summary = "\n\n".join(errors)
            telegram_send(f"ğŸ“‹ í¬ë¡¤ëŸ¬ ì‹¤í–‰ ì™„ë£Œ ({len(errors)}ê°œ ì—ëŸ¬ ë°œìƒ)\n\n{summary}")
        except Exception as e:
            print(f"í…”ë ˆê·¸ë¨ ì—ëŸ¬ ì•Œë¦¼ ì „ì†¡ ì‹¤íŒ¨: {e}")

if __name__ == "__main__":
    main()